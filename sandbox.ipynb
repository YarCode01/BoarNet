{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import ops\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn, ssd300_vgg16, ssdlite320_mobilenet_v3_large\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.transforms.functional as F\n",
    "from dataset import BoarDataset\n",
    "from boar_utils import show_image_with_boxes, nms\n",
    "import os\n",
    "import json\n",
    "from boar_utils import get_current_time\n",
    "INDEX = 11\n",
    "CURRENT_TIME = \"15-05-2024 16-55-34\"\n",
    "RESULTS_PATH = f\"results/predictions/{CURRENT_TIME}/\"\n",
    "# os.mkdir(RESULTS_PATH)\n",
    "MODEL_PATH = f\"models/{CURRENT_TIME}/epoch_17_fasterrcnn_mobilenet_v3_large_fpn.pth\"\n",
    "THRESHOLD = 0.2\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(num_classes=2, weights_backbone=torchvision.models.MobileNet_V3_Large_Weights.DEFAULT, trainable_backbone_layers=0)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "TEST_IMG_PATH = \"data/test/images\"\n",
    "TEST_ANNOTATION_PATH = \"data/test/labels\"\n",
    "model.eval()\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalization is not needed as it is done by pytorch by default\n",
    "    #                      std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_dataset = BoarDataset(img_path=TEST_IMG_PATH, annotation_path=TEST_ANNOTATION_PATH,transform=train_transform, yolo_format=True)\n",
    "test_dataset_vis = BoarDataset(img_path=TEST_IMG_PATH, annotation_path=TEST_ANNOTATION_PATH, yolo_format=True)\n",
    "images = [test_dataset[INDEX]['image']]\n",
    "boxes = model(images)\n",
    "# boxes_score_zip = zip(boxes[0]['boxes'], boxes[0]['scores'])\n",
    "# boxes = [box.tolist() for box, score in boxes_score_zip if score > THRESHOLD]\n",
    "# show_image_with_boxes(test_dataset_vis[INDEX]['image'], boxes)#, save_path=RESULTS_PATH+f\"{INDEX}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supressed_boxes = nms(bboxes=boxes[0]['boxes'], confidence_scores=boxes[0]['scores'],confidence_threshold=0.3, iou_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supressed_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_with_boxes(test_dataset_vis[INDEX]['image'], supressed_boxes['boxes'].tolist())#, save_path=RESULTS_PATH+f\"{INDEX}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = test_dataset_vis[INDEX]['target']\n",
    "predictions = supressed_boxes\n",
    "targets, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "metric = MeanAveragePrecision(iou_type=\"bbox\", box_format='xyxy')\n",
    "metric.update([predictions], [targets])\n",
    "from pprint import pprint\n",
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "map_metric = MeanAveragePrecision(box_format='xyxy', iou_type='bbox')\n",
    "\n",
    "# Iterate over the dataset and calculate predictions\n",
    "for i in range(len(test_dataset)):\n",
    "    print(i)\n",
    "    images = [test_dataset[i]['image']]\n",
    "    targets = test_dataset[i]['target']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "    boxes = model(images)\n",
    "    supressed_boxes = nms(bboxes=boxes[0]['boxes'], confidence_scores=boxes[0]['scores'],confidence_threshold=0.3, iou_threshold=0.3)\n",
    "    # Update the metric with current batch\n",
    "    map_metric.update([supressed_boxes], [targets])\n",
    "\n",
    "# Compute the final mAP score\n",
    "map_result = map_metric.compute()\n",
    "print(map_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
